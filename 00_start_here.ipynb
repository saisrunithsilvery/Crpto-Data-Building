{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623743e8-2cd7-47c6-99e7-100979384579",
   "metadata": {
    "collapsed": false,
    "name": "md_intro"
   },
   "source": [
    "# Assignment - 3 -Part-B : Cryptocurrency Prices\n",
    "\n",
    "* Author: Vishal Prasanna\n",
    "* Last Updated: 2/25/2025\n",
    "\n",
    "Welcome to the beginning of the Quickstart! Please refer to [the official Snowflake Notebook Data Engineering Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#0) for all the details including set up steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6273e5-bcf7-4492-92f6-cc161da082c6",
   "metadata": {
    "collapsed": false,
    "name": "md_step03"
   },
   "source": [
    "## Step 03 Setup Snowflake\n",
    "\n",
    "During this step we will create our CRYPTO environment. Update the SQL variables below with your GitHub username and Personal Access Token (PAT) as well as with your forked GitHub repository information.\n",
    "\n",
    "**Important**: Please make sure you have created the `dev` branch in your forked repository before continuing here. For instructions please see [Step 2 in the Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898c514-831d-4aa7-9697-004994953950",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_set_context"
   },
   "outputs": [],
   "source": [
    "SET MY_USER = CURRENT_USER();\n",
    "\n",
    "SET GITHUB_SECRET_USERNAME = 'vishalprasanna';\n",
    "SET GITHUB_SECRET_PASSWORD = 'personal access token';\n",
    "SET GITHUB_URL_PREFIX = 'https://github.com/vishalprasanna11';\n",
    "SET GITHUB_REPO_ORIGIN = 'https://github.com/VishalPrasanna11/DAMG-Assignment3B.git';\n",
    "SET RAPIDAPI_KEY = \"rapidapi key\";\n",
    "SET AWS_ACCESS_KEY_ID = \"aws access key id\";\n",
    "SET AWS_SECRET_ACCESS_KEY = \"aws secret access key\";\n",
    "SET AWS_REGION = \" aws region\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc608c96-0957-47e1-8492-bc8d382925e3",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_account_objects"
   },
   "outputs": [],
   "source": [
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the account level objects (ACCOUNTADMIN part)\n",
    "-- ----------------------------------------------------------------------------\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "-- Roles\n",
    "CREATE OR REPLACE ROLE CRYPTO_ROLE;\n",
    "GRANT ROLE CRYPTO_ROLE TO ROLE SYSADMIN;\n",
    "GRANT ROLE CRYPTO_ROLE TO USER IDENTIFIER($MY_USER);\n",
    "\n",
    "GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE CRYPTO_ROLE;\n",
    "GRANT EXECUTE TASK ON ACCOUNT TO ROLE CRYPTO_ROLE;\n",
    "GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE CRYPTO_ROLE;\n",
    "GRANT MONITOR EXECUTION ON ACCOUNT TO ROLE CRYPTO_ROLE;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE CRYPTO_ROLE;\n",
    "\n",
    "-- Databases\n",
    "CREATE OR REPLACE DATABASE CRYPTO_DB;\n",
    "GRANT OWNERSHIP ON DATABASE CRYPTO_DB TO ROLE CRYPTO_ROLE;\n",
    "\n",
    "-- Warehouses\n",
    "CREATE OR REPLACE WAREHOUSE CRYPTO_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\n",
    "GRANT OWNERSHIP ON WAREHOUSE CRYPTO_WH TO ROLE CRYPTO_ROLE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2ae2c-241b-4d8f-aa99-11a35f9833a4",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_database_objects"
   },
   "outputs": [],
   "source": [
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the database level objects\n",
    "-- ----------------------------------------------------------------------------\n",
    "USE ROLE CRYPTO_ROLE;\n",
    "USE WAREHOUSE CRYPTO_WH;\n",
    "USE DATABASE CRYPTO_DB;\n",
    "\n",
    "-- Schemas\n",
    "CREATE OR REPLACE SCHEMA INTEGRATIONS;\n",
    "CREATE OR REPLACE SCHEMA DEV_SCHEMA;\n",
    "CREATE OR REPLACE SCHEMA PROD_SCHEMA;\n",
    "\n",
    "USE SCHEMA INTEGRATIONS;\n",
    "\n",
    "-- External Frostbyte objects\n",
    "CREATE OR REPLACE STAGE FROSTBYTE_RAW_STAGE\n",
    "    URL = 's3://damg7245-crypto/raw_data/'\n",
    ";\n",
    "\n",
    "-- Secrets (schema level)\n",
    "CREATE OR REPLACE SECRET CRYPTO_GITHUB_SECRET\n",
    "  TYPE = password\n",
    "  USERNAME = $GITHUB_SECRET_USERNAME\n",
    "  PASSWORD = $GITHUB_SECRET_PASSWORD;\n",
    "  \n",
    "CREATE OR REPLACE SECRET CRYPTO_API_SECRET\n",
    "  TYPE = PASSWORD\n",
    "  SECRET_STRING = $RAPIDAPI_KEY; \n",
    "\n",
    "CREATE OR REPLACE SECRET CRYPTO_AWS_SECRET\n",
    "  TYPE = aws\n",
    "  AWS_ACCESS_KEY_ID = $AWS_ACCESS_KEY_ID\n",
    "  AWS_SECRET_ACCESS_KEY = $AWS_SECRET_ACCESS_KEY\n",
    "  AWS_REGION = $AWS_REGION;\n",
    "\n",
    "-- API Integration (account level)\n",
    "-- This depends on the schema level secret!\n",
    "\n",
    "CREATE OR REPLACE API INTEGRATION CRYPTO_GITHUB_API_INTEGRATION\n",
    "  API_PROVIDER = GIT_HTTPS_API\n",
    "  API_ALLOWED_PREFIXES = ($GITHUB_URL_PREFIX)\n",
    "  ALLOWED_AUTHENTICATION_SECRETS = (CRYPTO_GITHUB_SECRET)\n",
    "  ENABLED = TRUE;\n",
    "\n",
    "-- Git Repository\n",
    "CREATE OR REPLACE GIT REPOSITORY CRYPTO_GIT_REPO\n",
    "  API_INTEGRATION = CRYPTO_GITHUB_API_INTEGRATION\n",
    "  GIT_CREDENTIALS = CRYPTO_GITHUB_SECRET\n",
    "  ORIGIN = $GITHUB_REPO_ORIGIN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f26add-547e-4d60-8897-d5ad79b3311d",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_event_table"
   },
   "outputs": [],
   "source": [
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the event table\n",
    "-- ----------------------------------------------------------------------------\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE EVENT TABLE CRYPTO_DB.INTEGRATIONS.CRYPTO_EVENTS;\n",
    "GRANT SELECT ON EVENT TABLE CRYPTO_DB.INTEGRATIONS.CRYPTO_EVENTS TO ROLE CRYPTO_ROLE;\n",
    "GRANT INSERT ON EVENT TABLE CRYPTO_DB.INTEGRATIONS.CRYPTO_EVENTS TO ROLE CRYPTO_ROLE;\n",
    "\n",
    "ALTER ACCOUNT SET EVENT_TABLE = CRYPTO_DB.INTEGRATIONS.CRYPTO_EVENTS;\n",
    "ALTER DATABASE CRYPTO_DB SET LOG_LEVEL = INFO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531119f-76fc-4a2f-a635-a5a7526ac152",
   "metadata": {
    "collapsed": false,
    "name": "md_step04_deploy_dev_notebooks"
   },
   "source": [
    "## Step 04 Deploy to Dev\n",
    "\n",
    "Finally we will use `EXECUTE IMMEDIATE FROM <file>` along with Jinja templating to deploy the Dev version of our Notebooks. We will directly execute the SQL script `prototypes/vishal/scripts/deploy_notebooks.sql` from our Git repository which has the SQL commands to deploy a Notebook from a Git repo.\n",
    "\n",
    "See [EXECUTE IMMEDIATE FROM](https://docs.snowflake.com/en/sql-reference/sql/execute-immediate-from) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8676d0-7f82-4639-a5e2-29f7f9dca0f5",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_step04_deploy_dev_notebooks"
   },
   "outputs": [],
   "source": [
    "USE ROLE CRYPTO_ROLE;\n",
    "USE WAREHOUSE CRYPTO_WH;\n",
    "USE SCHEMA CRYPTO_DB.INTEGRATIONS;\n",
    "\n",
    "EXECUTE IMMEDIATE FROM @CRYPTO_GIT_REPO/branches/main/prototypes/vishal/scripts/deploy_notebooks.sql\n",
    "    USING (env => 'DEV', branch => 'dev');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bb327-95e4-4559-b7c7-f034607196c9",
   "metadata": {
    "collapsed": false,
    "name": "md_step05"
   },
   "source": [
    "## Step 05 Load the CRYPTO DATA\n",
    "\n",
    "But what about data that needs constant updating - like the CRYPTO DATA data? We would need to build a pipeline process to constantly update that data to keep it fresh.\n",
    "\n",
    "Perhaps a better way to get this external data would be to source it from a trusted data supplier. Let them manage the data, keeping it accurate and up to date.\n",
    "\n",
    "Enter the Snowflake Data Cloud...\n",
    "\n",
    "Weather Source is a leading provider of global weather and climate data and their OnPoint Product Suite provides businesses with the necessary weather and climate data to quickly generate meaningful and actionable insights for a wide range of use cases across industries. Let's connect to the \"Weather Source LLC: frostbyte\" feed from Weather Source in the Snowflake Data Marketplace by following these steps in Snowsight\n",
    "\n",
    "* In the left navigation bar click on \"Data Products\" and then \"Marketplace\"\n",
    "* Search: \"Weather Source LLC: frostbyte\" (and click on tile in results)\n",
    "* Click the blue \"Get\" button\n",
    "* Under \"Options\", adjust the Database name to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n",
    "* Grant to \"HOL_ROLE\"\n",
    "\n",
    "That's it... we don't have to do anything from here to keep this data updated. The provider will do that for us and data sharing means we are always seeing whatever they they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a850e3-44a4-4829-882e-84724f7e77d7",
   "metadata": {
    "language": "sql",
    "name": "sql_step05_create_share"
   },
   "outputs": [],
   "source": [
    "/*---\n",
    "-- You can also do it via code if you know the account/share details...\n",
    "SET WEATHERSOURCE_ACCT_NAME = '*** PUT ACCOUNT NAME HERE AS PART OF CRYPTO SETUP ***';\n",
    "SET WEATHERSOURCE_SHARE_NAME = '*** PUT ACCOUNT SHARE HERE AS PART OF CRYPTO SETUP ***';\n",
    "SET WEATHERSOURCE_SHARE = $WEATHERSOURCE_ACCT_NAME || '.' || $WEATHERSOURCE_SHARE_NAME;\n",
    "\n",
    "CREATE OR REPLACE DATABASE FROSTBYTE_WEATHERSOURCE\n",
    "  FROM SHARE IDENTIFIER($WEATHERSOURCE_SHARE);\n",
    "\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE FROSTBYTE_WEATHERSOURCE TO ROLE HOL_ROLE;\n",
    "---*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2762d1-fe91-4a7c-b89a-56e1baf0001c",
   "metadata": {
    "language": "sql",
    "name": "sql_step05_test_share"
   },
   "outputs": [],
   "source": [
    "-- Let's look at the data - same 3-part naming convention as any other table\n",
    "SELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d93974-9a75-46c2-876f-95b6e1562f75",
   "metadata": {
    "collapsed": false,
    "name": "md_step06"
   },
   "source": [
    "## Step 06 Load Excel Files\n",
    "\n",
    "Please follow the instructions in [Step 6 of the Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#5) to open and run the `DEV_06_load_excel_files` Notebook. That Notebook will define the pipeline used to load data into the `LOCATION` and `ORDER_DETAIL` tables from the staged Excel files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5587283-a264-444d-b9ec-55b4d926a5c7",
   "metadata": {
    "collapsed": false,
    "name": "md_step07"
   },
   "source": [
    "## Step 07 Load Daily City Metrics\n",
    "\n",
    "Please follow the instructions in [Step 7 of the Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#6) to open and run the `DEV_07_load_daily_city_metrics` Notebook. That Notebook will define the pipeline used to create the `DAILY_CITY_METRICS` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfff6b-067e-4f24-8424-19d0231c0ee1",
   "metadata": {
    "language": "sql",
    "name": "sql_step07_logs"
   },
   "outputs": [],
   "source": [
    "USE ROLE CRYPTO_ROLE;\n",
    "USE WAREHOUSE CRYPTO_WH;\n",
    "USE SCHEMA CRYPTO_DB.INTEGRATIONS;\n",
    "\n",
    "SELECT TOP 100\n",
    "  RECORD['severity_text'] AS SEVERITY,\n",
    "  VALUE AS MESSAGE\n",
    "FROM\n",
    "  CRYPTO_DB.INTEGRATIONS.CRYPTO_EVENTS\n",
    "WHERE 1 = 1\n",
    "  AND SCOPE['name'] = 'CRYPTO_logger'\n",
    "  AND RECORD_TYPE = 'LOG';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873a1db-1fbe-4a44-b02a-03e1b2084cb2",
   "metadata": {
    "collapsed": false,
    "name": "md_step08"
   },
   "source": [
    "## Step 08 Orchestrate Pipelines\n",
    "\n",
    "In this step we will create a DAG (or Directed Acyclic Graph) of Tasks using the new [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview). The Task DAG API builds upon the Python Management API to provide advanced Task management capabilities. For more details see [Managing Snowflake tasks and task graphs with Python](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-managing-tasks).\n",
    "\n",
    "This code is also available in the `scripts/deploy_task_dag.py` script which could be used to automate the Task DAG deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac9ad2-2858-4fb7-b3a2-6394db5b0b4c",
   "metadata": {
    "language": "python",
    "name": "py_step08_imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from snowflake.core import Root\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "session.use_role(\"CRYPTO_ROLE\")\n",
    "session.use_warehouse(\"CRYPTO_WH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bbc8e-c525-4cd0-b147-a832a9060c47",
   "metadata": {
    "language": "python",
    "name": "py_step08_set_env"
   },
   "outputs": [],
   "source": [
    "database_name = \"CRYPTO_DB\"\n",
    "schema_name = \"DEV_SCHEMA\"\n",
    "#schema_name = \"PROD_SCHEMA\"\n",
    "env = 'PROD' if schema_name == 'PROD_SCHEMA' else 'DEV'\n",
    "\n",
    "session.use_schema(f\"{database_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c030976-5888-4d9f-a329-3248b25abd78",
   "metadata": {
    "language": "python",
    "name": "py_step08_create_dag"
   },
   "outputs": [],
   "source": [
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the tasks using the DAG API\n",
    "warehouse_name = \"CRYPTO_WH\"\n",
    "dag_name = \"CRYPTO_DAG\"\n",
    "\n",
    "api_root = Root(session)\n",
    "schema = api_root.databases[database_name].schemas[schema_name]\n",
    "dag_op = DAGOperation(schema)\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(dag_name, schedule=timedelta(days=1), warehouse=warehouse_name) as dag:\n",
    "    dag_task1 = DAGTask(\"LOAD_EXCEL_FILES_TASK\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_06_load_excel_files\"()''', warehouse=warehouse_name)\n",
    "    dag_task2 = DAGTask(\"LOAD_DAILY_CITY_METRICS\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_07_load_daily_city_metrics\"()''', warehouse=warehouse_name)\n",
    "\n",
    "    # Define the dependencies between the tasks\n",
    "    dag_task1 >> dag_task2 # dag_task1 is a predecessor of dag_task2\n",
    "\n",
    "# Create the DAG in Snowflake\n",
    "dag_op.deploy(dag, mode=\"orreplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560c909-5523-4a12-ad21-0b9044bdaff6",
   "metadata": {
    "language": "python",
    "name": "py_step08_run_dag"
   },
   "outputs": [],
   "source": [
    "dagiter = dag_op.iter_dags(like='CRYPTO_dag%')\n",
    "for dag_name in dagiter:\n",
    "    print(dag_name)\n",
    "\n",
    "#dag_op.run(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad46ffc-1137-43dc-add8-7fc02914bbaa",
   "metadata": {
    "collapsed": false,
    "name": "md_step09"
   },
   "source": [
    "## Step 09 Deploy to Production\n",
    "\n",
    "Steps\n",
    "1. Make a small change to a notebook and commit it to the dev branch\n",
    "1. Go into GitHub and create a PR and Merge to main branch\n",
    "1. Review GitHub Actions workflow definition and run results\n",
    "1. See new \"PROD_\" versions of the Notebooks\n",
    "1. Deploy the production version of the task DAG\n",
    "1. Run production version of the task DAG and see new tables created!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba497c01-0988-4c07-af66-79ee2918cffa",
   "metadata": {
    "collapsed": false,
    "name": "md_step10"
   },
   "source": [
    "## Step 10 Teardown\n",
    "\n",
    "Finally, we will tear down our CRYPTO environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ca116-4585-4668-bb72-cf74b0e7b587",
   "metadata": {
    "language": "sql",
    "name": "sql_step10"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "DROP API INTEGRATION CRYPTO_GITHUB_API_INTEGRATION;\n",
    "DROP DATABASE CRYPTO_DB;\n",
    "DROP WAREHOUSE CRYPTO_WH;\n",
    "DROP ROLE CRYPTO_ROLE;\n",
    "\n",
    "-- Drop the weather share\n",
    "DROP DATABASE FROSTBYTE_WEATHERSOURCE;\n",
    "\n",
    "-- Remove the \"dev\" branch in your repo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
